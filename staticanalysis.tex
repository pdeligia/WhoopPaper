After the original program has been translated to the abstract Boogie program, the core \whoop tool is invoked, which performs \emph{static pairwise lockset analysis}, a novel verification technique for proving data race freedom in Linux kernel modules. The key idea behind our approach is that we can verify absence of data races by (i) deriving a sound \emph{sequential} model that \emph{over-approximates} the originally concurrent program, (ii) instrumenting it for lockset analysis and race checking, and (iii) asserting that all write/read accesses to the same shared resource are consistently protected by at least one common lock. The immediate benefit is that our approach not only avoids reasoning about thread interleavings, and thus has the potential to scale well, but also allows the reuse of existing sequential verification techniques.

The proof sketch behind our static lockset analysis is given in Section~\ref{tech:proof}. The instrumentation and analysis is described in more detail in Section~\ref{tech:method}. The summarisation that we perform to achieve scalability is discussed in Section~\ref{tech:summarisation}. Various optimisations that we perform to achieve precision are described in Section~\ref{tech:optimisations}. Finally, assumption and limitations are discussed in Section~\ref{tech:limitations}.

\subsection{Static Lockset Analysis Proof Sketch}
\label{tech:proof}

We now formalise our approach to verifying data race freedom in a kernel module by statically analysing its locksets. Let $\{\mathit{ep}_{1}, \mathit{ep}_{2}, \dotsc, \mathit{ep}_{n}\}$ be all the entry points of a module, and let $\{\ell_{1}, \ell_{2}, \dotsc, \ell_{k}\}$ be all the locks used by this module. Although we assume that the number of locks is finite and with a known bound, we argue that this assumption is realistic: all modules that we have studied so far in the Linux kernel have only a small number of locks (typically one). Indeed, the Linux device driver book~\cite{corbet2005linux} advocates the use of as few locks as possible to avoid introducing unnecessary complexity and synchronisation bugs that arise from careless fine-grained locking. The actual Linux kernel employs sophisticated fine-grained locking, but this is part of the environment which is abstracted away in our models.

For each entry point $\mathit{ep}_{i}$, we compute its lockset $\mathit{LS}_{i}$ that associates each memory location $m$ to the set of locks that are always held when $m$ is accessed during the execution of $\mathit{ep}_{i}$. Furthermore, for each entry point $\mathit{ep}_{i}$, we compute its read-set $R_{i}\lbrack m\rbrack \rightarrow \{true, false\}$ and its write-set $W_{i}\lbrack m\rbrack \rightarrow \{true, false\}$, which map each $m$ to true if and only if $\mathit{ep}_{i}$ reads or writes respectively to $m$ during some execution. In the actual implementation we avoid the use of quantifiers by using a different lockset, read-set and write-set for each shared memory location. \PDComment{any concrete reference that quantifiers might be harder for SMT solving?}

\begin{theorem}
\label{theorem:locksets}
For each pair of entry points $\mathit{ep}_{i}, \mathit{ep}_{j}\in \{\mathit{ep}_{1}, \mathit{ep}_{2}, ..., \mathit{ep}_{n}\}$, where $i$ may be equal to $j$, and for each memory location $m$, if $(W_{i}\lbrack m\rbrack \vee W_{j}\lbrack m\rbrack) \wedge (W_{i}\lbrack m\rbrack \vee R_{j}\lbrack m\rbrack) \wedge (R_{i}\lbrack m\rbrack \vee W_{j}\lbrack m\rbrack) \implies (\mathit{LS}_{i}\lbrack m\rbrack \cap \mathit{LS}_{j}\lbrack m\rbrack \not= \varnothing)$, then the kernel module with entry points $\{\mathit{ep}_{1}, \mathit{ep}_{2}, \dotsc, \mathit{ep}_{n}\}$ is free from data races.
\end{theorem}

A sketch of how the above theorem can be proved is as follows. Suppose there are in fact entry points $\mathit{ep}_{i}$ and $\mathit{ep}_{j}$ that can race on a memory location $m$. By our hypothesis, there exists at least one lock, say $\ell$, which belongs to both $\mathit{LS}_{i}$ and $\mathit{LS}_{j}$. By the definition of a lockset, this means that $\ell$ is held during the access to $m$ by both $ep_1$ and $ep_2$. As a result, $m$ \emph{must} be unlocked and locked between the two accesses, which contradicts that the pair of accesses is racing.

\subsection{Instrumentation and Analysis}
\label{tech:method}

The Engine component of \whoop (see Figure~\ref{fig:whoop}) is responsible for parsing, instrumenting and sequentialising the abstract Boogie program. During the initial parsing, \whoop performs a static analysis (on the Boogie IVL code) to identify all lock identifiers and rewrite them to a unique constant Boogie variable, e.g. the following call to \texttt{mutex\_lock()}:

\begin{boogie}
call mutex_lock($p3);
\end{boogie}

Will be rewritten to:

\begin{boogie}
const {:lock} unique lock$0: int;

call mutex_lock(lock$0);
\end{boogie}

If \whoop cannot infer a lock it will exit with a warning. A limitation of lockset analysis is that it cannot understand external locking mechanisms (\whoop currently only supports Linux kernel mutexes and spinlocks). It is relatively straightforward to enhance \whoop, though, with new locking primitives. \whoop will also exit with a warning if it detects ``improper'' use of locks (e.g. dynamic lock creation). The reason behind this is twofold: first, it is arguably infeasible to detect such locks using static analysis; and second, because we want to advocate the use of good locking practises when developing modules for the Linux kernel.

When the lock rewriting finishes, \whoop traverses the Boogie IVL code, uses the information extracted by Chauffeur and separates each independent entry point call-graph from the rest of the program. To achieve this, it duplicates helper functions and renames them accordingly: e.g. for entry point \texttt{ep1}, a helper function \texttt{foo} will be renamed to \texttt{foo\$ep1}. This allows \whoop to perform entry point sensitive instrumentation and analysis. Although the program can potentially become much larger, this does not affect the analysis as discussed later.

Next, \whoop instruments the program with global variables for lockset analysis and race checking per entry point. For example, for entry point \texttt{ep1}, \whoop instruments:

\begin{boogie}
var {:current_lockset} lock$0_in_CLS_$ep1: bool;
var {:lockset} lock$0_in_LS_$M.0_$ep1: bool;
var {:access_checking} WRITTEN_$M.0_$ep1: bool;
var {:access_checking} READ_$M.0_$ep1: bool;
\end{boogie}

\whoop also instruments a global watchdog variable per memory region that is common to all entry points:

\begin{boogie}
const {:watchdog} WATCHED_ACCESS_$M.0: int;
\end{boogie}

The above instrumented variables represent the follows: \texttt{lock\$0\_in\_CLS\_\$ep1} is the current lockset (CLS) of \texttt{ep1}; \texttt{WATCHED\_ACCESS\_\$M.0} is an \emph{unconstrained} constant representing the offset to memory region \texttt{\$M.0} that should be checked for races; \texttt{lock\$0\_in\_LS\_\$M.0\_\$ep1} is the lockset for \texttt{\$M.0}; \texttt{WRITTEN\_\$M.0\_\$ep1} is the write-set for \texttt{\$M.0}; and \texttt{READ\_\$M.0\_\$ep1} is the read-set for \texttt{\$M.0}. Verification involves proving that two entry points cannot race at the watched offset of every memory region. The arbitrary watched offset implies that every offset of each memory region is race-free. Watchdog race checking has been used before for verifying GPU kernels~\cite{bardsley2014engineering}.

\whoop proceeds by instrumenting each entry point call-graph for lockset analysis by replacing each call to a locking or unlocking function with a \whoop-specific lock and unlock functions. This means that Example~\ref{fig:smack_translation} will become:

\begin{boogie}
procedure ep1(dev: int) modifies $M.0; {
  var $p0, $p1, $p2, $p3, $p4, $p5, $p6: int;
  $bb0:
    $p0 := dev;
    $p1 := $pa($p0, 32, 1);
    $p2 := $pa($pa($p1, 0, 24), 12, 1);
    $p3 := $pa($pa($p2, 0, 12), 0, 1);
    call _UPDATE_CLS_$ep1(lock$0, true);
    $p4 := $pa($pa($p1, 0, 24), 4, 1);
    $M.0[$p4] := 4;
    $p5 := $pa($pa($p1, 0, 24), 12, 1);
    $p6 := $pa($pa($p5, 0, 12), 0, 1);
    call _UPDATE_CLS_$ep1(lock$0, false);
    return;
}
\end{boogie}

The \texttt{\_UPDATE\_CLS\_\$ep1()} is a special function that updates the CLS global variable to either true or false, if the lock is held or released by \texttt{ep1} accordingly:

\begin{boogie}
procedure {:inline 1} _UPDATE_CLS_$ep1(lock: int,
    isLocked: bool);
  modifies lock$0_in_CLS_$ep1;

implementation {:inline 1} _UPDATE_CLS_$ep1(
    lock: int, isLocked: bool) {  _UPDATE:    lock$0_in_CLS_$ep1 := (if lock == lock$0 then
      isLocked else lock$0_in_CLS_$ep1);
  return;}
\end{boogie}

Next, \whoop replaces every write and read access in each entry point call-graph with a \whoop-specific function:

\begin{boogie}
procedure ep1(dev: int) modifies $M.0; {
  var $p0, $p1, $p2, $p3, $p4, $p5, $p6: int;
  $bb0:
    \textbf{$p0 := dev;}
    $p1 := $pa($p0, 32, 1);
    $p2 := $pa($pa($p1, 0, 24), 12, 1);
    $p3 := $pa($pa($p2, 0, 12), 0, 1);
    call _UPDATE_CLS_$ep1(lock$0, true);
    $p4 := $pa($pa($p1, 0, 24), 4, 1);
    call _WRITE_LS_$M.0_$ep1($p4);
    $p5 := $pa($pa($p1, 0, 24), 12, 1);
    $p6 := $pa($pa($p5, 0, 12), 0, 1);
    call _UPDATE_CLS_$ep1(lock$0, false);
    return;
}
\end{boogie}

The \texttt{\_WRITE\_LS\_\$M.0\_\$ep1()} is a special function that computes the intersection of the CLS and the lockset of \texttt{\$M.0} (see Section~\ref{bg:lockset}). It also updates the write-set of \texttt{\$M.0} to true, because there was a write access by \texttt{ep1}:

\begin{boogie}
procedure {:inline 1} _WRITE_LS_$M.0_$ep1(
    ptr: int);  modifies lock$0_in_LS_$M.0_$ep1,
    WRITTEN_$M.0_$ep1;

implementation {:inline 1} _WRITE_LS_$M.0_$ep1(
    ptr: int) {  _WRITE:    goto anon1_Then, anon1_Else;

  anon1_Then:    assume {:partition} WATCHED_ACCESS_$M.0 ==
      ptr && DEVICE_IS_REGISTERED_$ep;
    lock$0_in_LS_$M.0_$ep := lock$0_in_CLS_$ep && lock$0_in_LS_$M.0_$ep;    WRITTEN_$M.0_$ep := true;
    return;  anon1_Else:    assume {:partition} !(WATCHED_ACCESS_$M.0 ==
      ptr && DEVICE_IS_REGISTERED_$ep);
    return;
}
\end{boogie}

The \texttt{\_READ\_LS\_\$M.0\_\$ep1()} updates the lockset of \texttt{\$M.0} and the read-set accordingly for a read access.

\whoop continues by performing \emph{two-thread reduction}, a sound abstraction that removes all but two arbitrary threads, each running an entry point of the originally concurrent program, and then performs \emph{pairwise sequentialisation}, which combines the two arbitrary threads in a single sequential pair. This is achieved by creating a checker function that calls the two instrumented entry points and then performs race checking assertions. This process repeats until all possible pairs of entry points have been sequentialised. To be sound, each time an entry point performs a read access to a shared memory location, \whoop returns a nondeterministic value (using the Boogie keyword \texttt{havoc}). This over-approximates any effects from all the unmodeled threads on the shared state of the pair. Two-thread reduction is not a new idea, it has been used before in GPU kernel verification~\cite{bardsley2014engineering} and in model checking of cache coherence protocols~\cite{mcmillan1999verification}.

After each checker function calls a pair of entry points, it asserts that each shared memory location is protected by at least one common lock (see Section~\ref{bg:lockset}). The assertion check is triggered iff the memory location was accessed by both entry points and at least one of the accesses was a write. We do not check for read-read races, which are inherently benign. The following is an example of a checker function for entry points \texttt{ep1} and \texttt{ep2}:

\begin{boogie}
procedure check$ep1$ep2(dev: int);
  requires !lock$0_in_CLS_$ep1;  requires !lock$0_in_CLS_$ep2;  requires lock$0_in_LS_$M.0_$ep1;  requires lock$0_in_LS_$M.0_$ep2;  requires !WRITTEN_$M.0_$ep1;  requires !WRITTEN_$M.0_$ep2;  requires !READ_$M.0_$ep1;  requires !READ_$M.0_$ep2;  modifies lock$0_in_CLS_$ep1, lock$0_in_CLS_$ep2,
    lock$0_in_LS_$M.0_$ep1, lock$0_in_LS_$M.0_$ep2,
    WRITTEN_ $M.0_$ep1,WRITTEN_$M.0_$ep2,
    READ_$M.0_$ep1, READ_$M.0_$ep2;

implementation check$ep1$ep2(dev: int) {  _CHECK:
    call ep1(dev);
    call ep2(dev);    assert (WRITTEN_$M.0_$ep1 && WRITTEN_$M.0_$ep2)
      || (WRITTEN_$M.0_$ep1 && READ_$M.0_$ep2)
      || (READ_$M.0_$ep1 && WRITTEN_$M.0_$ep2)
      ==> lock$0_in_LS_$M.0_$ep1 &&
      lock$0_in_LS_$M.0_$ep2;    return;
}
\end{boogie}

Finally the abstract program is send to the Boogie verification engine, which generates verification conditions~\cite{barnett2005weakest} and discharges them to a theorem prover (Boogie supports Z3~\cite{de2008z3} and CVC4~\cite{barrett2011cvc4}). Successful verification implies that the original program is free of data races, while an error denotes a \emph{potential} data race and is reported to the user. To improve usability \whoop has a built-in error reporter that matches Boogie counterexamples to source code. The following is an example of a reported error:

\begin{lstlisting}
foo.c: error: potential write-write race:

write by entry point ep1, foo.c:36:2
  shared->resource = 4;

write by entry point ep2, foo.c:52:2
  shared->resource = 5;
\end{lstlisting}

\subsection{Watchdog Summarisation}
\label{tech:summarisation}

Draft.

\subsection{Optimisations}
\label{tech:optimisations}

Draft.

\subsection{Assumptions and Limitations}
\label{tech:limitations}
